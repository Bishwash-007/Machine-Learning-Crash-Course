{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic Regression (Classification)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Logistic regression is a statistical and machine learning technique for modeling the probability of a binary outcome (two possible classes) based on one or more predictor variables. It is widely used in classification tasks such as spam detection, disease diagnosis, and predicting customer churn.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Logistic Function\n",
    "\n",
    "The logistic regression model uses the logistic (or sigmoid) function to predict probabilities. The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Here, \\( z \\) is a linear combination of the independent variables.\n",
    "\n",
    "#### Model Equation\n",
    "\n",
    "The logistic regression model predicts the probability \\( P(y=1|x) \\) as:\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)\n",
    "$$\n",
    "\n",
    "- $ y $: Dependent variable (binary: 0 or 1).\n",
    "- $ x $: Independent variables (features).\n",
    "- $ \\beta_0 $ : Intercept.\n",
    "-$  \\beta_1, \\beta_2, \\ldots, \\beta_n$ : Coefficients of the independent variables.\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "The log-odds (logit function) is:\n",
    "\n",
    "$$\n",
    "\\text{logit}(P) = \\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "#### Likelihood Function\n",
    "\n",
    "The likelihood function maximizes the probability of observing the actual data. The likelihood for \\( n \\) observations is:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n P(y_i|x_i)^{y_i} (1 - P(y_i|x_i))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "#### Log-Likelihood\n",
    "\n",
    "To simplify, we take the natural logarithm of the likelihood (log-likelihood):\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log(P(y_i|x_i)) + (1 - y_i) \\log(1 - P(y_i|x_i)) \\right]\n",
    "$$\n",
    "\n",
    "#### Negative Log-Likelihood (Cost Function)\n",
    "\n",
    "The cost function to minimize becomes the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "J(\\beta) = -\\ell(\\beta) = -\\sum_{i=1}^n \\left[ y_i \\log(P(y_i|x_i)) + (1 - y_i) \\log(1 - P(y_i|x_i)) \\right]\n",
    "$$\n",
    "\n",
    "where $ P(y_i|x_i) $ is the predicted probability.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "To minimize the cost function $ J(\\beta) $, we use gradient descent. The partial derivative of $ J(\\beta) $ with respect to $ \\beta_j $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^n (P(y_i|x_i) - y_i) x_{ij}\n",
    "$$\n",
    "\n",
    "The parameter $ \\beta_j $ is updated iteratively:\n",
    "\n",
    "$$\n",
    "\\beta_j := \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\alpha $: Learning rate.\n",
    "- $ x_{ij} $: The $ j $-th feature of the $ i $-th example.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Steps of Logistic Regression:\n",
    "1. **Initialize Parameters**: Set initial values for $ \\beta $ (e.g., zeros).\n",
    "2. **Compute Predictions**: For each training example $ x_i $, compute the predicted probability:\n",
    "    $$\n",
    "    P(y_i|x_i) = \\sigma(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_n x_{in})\n",
    "    $$\n",
    "3. **Calculate Cost**: Compute the cost function $ J(\\beta) $ using the negative log-likelihood.\n",
    "4. **Compute Gradients**: Compute the gradient of $J(\\beta) $ with respect to each parameter $ \\beta_j $.\n",
    "5. **Update Parameters**: Update $ \\beta $ using gradient descent:\n",
    "    $$\n",
    "    \\beta_j := \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}\n",
    "    $$\n",
    "6. **Repeat**: Iterate over steps 2â€“5 until convergence (i.e., the cost function stops changing significantly).\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Logistic regression relies on these assumptions:\n",
    "1. **Binary Dependent Variable**: The output \\( y \\) is binary (0 or 1).\n",
    "2. **Independent Observations**: Observations are independent of each other.\n",
    "3. **Linearity of Logit**: The log-odds of the dependent variable are a linear function of the independent variables.\n",
    "4. **No Multicollinearity**: Independent variables are not highly correlated.\n",
    "5. **Large Sample Size**: A larger dataset improves parameter estimation.\n",
    "\n",
    "### Applications\n",
    "1. **Medical Diagnosis**: Predicting the presence or absence of a disease.\n",
    "2. **Spam Detection**: Classifying emails as spam or not spam.\n",
    "3. **Credit Scoring**: Assessing the likelihood of loan repayment.\n",
    "4. **Customer Churn**: Predicting whether a customer will leave a service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
